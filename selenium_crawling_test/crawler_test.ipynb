{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from urllib import parse\n",
    "\n",
    "\n",
    "# 사이드 메뉴 열 필요 없어서 일땐 빼놈. @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    "# 여기 참고해서 작성해봤었음. https://brunch.co.kr/@jk-lab/18\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 지역별로 버튼 클릭할 필요없이 걍 다 담겨있네..\n",
    "# from selenium import webdriver\n",
    "\n",
    "# chromedriver_dir = '/home/nasanmaro/Desktop/projects/yapen/test/selenium_crawling_test/chromedriver'\n",
    "# driver = webdriver.Chrome(chromedriver_dir)\n",
    "# driver.get('http://www.yapen.co.kr')\n",
    "# time.sleep(5)\n",
    "\n",
    "# # 왼쪽 메뉴를 연다. \n",
    "# loca = driver.find_element_by_class_name('yapenMenu')\n",
    "# loca.click()\n",
    "# time.sleep(5)\n",
    "\n",
    "# # 드라이버 인스턴스로부터 현제 메뉴 연상태의 페이지 소스 받아서 source에 넣는다. \n",
    "# source = driver.page_source\n",
    "\n",
    "# # soup 객체로 만듬.\n",
    "# soup = BeautifulSoup(source, 'lxml')\n",
    "\n",
    "\n",
    "\n",
    "# 이것까지 하면 사이드 메뉴여는것 됬다. \n",
    "##########뒤에 아직따라안한 부분.################################################\n",
    "\n",
    "\n",
    "# sido = driver.find_element_by_class_name('sido_area_box')\n",
    "# li = sido.find_elements_by_tag_name('li')\n",
    "# li[0].click()\n",
    "# time.sleep(5)\n",
    "\n",
    "# gugun = driver.find_element_by_class_name('gugun_area_box')\n",
    "# guli = gugun.find_element_by_tag_name('li')\n",
    "# guuli.click()\n",
    "# time.sleep(5)\n",
    "\n",
    "\n",
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "# location_name_list 뽑는 과정 \n",
    "\n",
    "def location_name_list_crawler():\n",
    "    request = requests.get(\"http://www.yapen.co.kr\")\n",
    "    response = request.text\n",
    "    soup = BeautifulSoup(response, 'lxml')\n",
    "\n",
    "    left_menu=soup.select('div.locLayer')\n",
    "    # 풀빌라, MD추천 제외 14지역중 7지역 만남김.\n",
    "    selected_left_menu= left_menu[2:9]\n",
    "    \n",
    "\n",
    "    # 여기에 list 형태로 지역,지역고유번호/(세부지역,고유번호) 넣고싶다. \n",
    "    # location_name_list =[ [지역1 ,지역1고유번호 , [ (고유번호,세부지역),(고유번호2,세부지역2)...] ],\n",
    "    #                       [지역2 ,지역2고유번호 , [ (고유번호,세부지역),(고유번호2,세부지역2)...] ],..\n",
    "    location_name_list=list()\n",
    "\n",
    "    for location in selected_left_menu:\n",
    "        # 지역 이름 먼저 뽑음 \n",
    "        location_name = location.select('div.titleStyle')[0].get_text(strip=True)\n",
    "        location_name_sub_list=list()\n",
    "        location_name_sub_list.append(location_name)\n",
    "\n",
    "        li=location.select('li')\n",
    "\n",
    "        #for문 돌면서 (고유번호 세부지역) 리스트에 담은뒤 location_list에 넣겠다.\n",
    "        location_detail_list=[] \n",
    "        for location_detail in li:\n",
    "            onclick_value = location_detail['onclick']  #regionMove('1.003021','금산/논산');\n",
    "            \n",
    "            split_right = onclick_value.split(',')[0]\n",
    "            split_left = onclick_value.split(',')[1]\n",
    "\n",
    "            sub_location_no = re.findall(\"'(.+)'\",split_right)[0]\n",
    "            sub_location_name = re.findall(\"'(.+)'\",split_left)[0]\n",
    "\n",
    "            location_detail_tuple = (sub_location_no,sub_location_name)\n",
    "\n",
    "             # (고유번호/세부지역) 리스트에 담음\n",
    "            location_detail_list.append(location_detail_tuple)\n",
    "        \n",
    "        # 지역 고유번호부터 정규표현식으로 뽑아내서 담음.(세부지역 고유번호의 소숫점 뒤 3자리)\n",
    "        \n",
    "        location_detail_no_for_search = location_detail_list[0][0] # '1.003021'\n",
    "        location_detaol_no = re.findall(\".(\\d\\d\\d)\",location_detail_no_for_search)[0]\n",
    "        location_name_sub_list.append(location_detaol_no)\n",
    "        \n",
    "        # location_name_sub_list (고유번호 세부지역) 리스트를 넣음.\n",
    "        location_name_sub_list.append(location_detail_list)\n",
    "        \n",
    "        # 상위 리스트에 넣음 \n",
    "        location_name_list.append(location_name_sub_list)\n",
    "        \n",
    "    return location_name_list\n",
    "    \n",
    "    \n",
    "# location_dict 뽑는 과정 끝\n",
    "######################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "#이제 각 페이지 location 가서 해커톤때 쓴 pension crawler 돌리고 싶다.\n",
    "# 메인페이지에서 기본정보 3개만 여러개 팬션에게서 가져왔던것.\n",
    "\n",
    "# url = \"http://www.yapen.co.kr/region?location=015&subLocation=1.015001\"\n",
    "\n",
    "def pension_crawler(location_no,sub_location_no):\n",
    "\n",
    "    params = {\n",
    "            'location':location_no,\n",
    "             'subLocation':sub_location_no,\n",
    "        }\n",
    "\n",
    "    url = \"http://www.yapen.co.kr/region?\" + parse.urlencode(params)\n",
    "\n",
    "    request = requests.get(url)\n",
    "    response = request.text\n",
    "    soup = BeautifulSoup(response, 'lxml')\n",
    "\n",
    "\n",
    "    title_list=list()\n",
    "    img_file_list=list()\n",
    "    price_list=list()\n",
    "    pldx_list=list()\n",
    "    discount_rate_list=list()\n",
    "\n",
    "    title_uls = soup.select('ul.dest-place-opt-fea')\n",
    "    for ul in title_uls:\n",
    "        li=ul.select('li')\n",
    "        title_list.append(li[1].get_text())\n",
    "\n",
    "    price_uls = soup.select('ul.dest-place-opt-cast')\n",
    "    for ul in price_uls:\n",
    "        li=ul.select('li')\n",
    "        price_list.append(li[1].get_text())\n",
    "\n",
    "\n",
    "    img_file_divs = soup.select('div.imgBox')\n",
    "    for div in img_file_divs:\n",
    "        img_file_list.append(div.select('img')[0]['src'])\n",
    "\n",
    "        list1 = re.split('/', div.select('img')[0]['src'])\n",
    "        pldx_list.append(int(list1[5]))\n",
    "\n",
    "\n",
    "    dest_place_pics = soup.select('div.dest-place-pic')\n",
    "    for dest_place_pic in dest_place_pics:\n",
    "        # dest_place_pic에는 dic가 2개 or 1게 있는데  discount_rate가 있는 경우는 div가 2개이며\n",
    "        # 길이가 5여서 이것으로 discount_rate있고 없고 를 비교한다. \n",
    "        if(len(dest_place_pic)==5):\n",
    "            discount_rate_string =dest_place_pic.select('div')[0].get_text(strip=True)\n",
    "            # %문자 정규표현식으로 빼줌.\n",
    "            discount_rate_int = int(re.search('(\\d*)',discount_rate_string).group())\n",
    "            discount_rate_list.append(discount_rate_int)\n",
    "        else:\n",
    "            discount_rate_list.append(0)\n",
    "\n",
    "\n",
    "    sub_locations_info_list = [title_list, price_list, img_file_list,pldx_list,discount_rate_list]\n",
    "    \n",
    "    return sub_locations_info_list\n",
    "        \n",
    "\n",
    "###########################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########################################################################################\n",
    "##province_name_list 로부터 지역명,고유번호 받아서 각 세부지역별로 pension_crawler \n",
    "## 해서 기본정보 3개씩 모으는 크롤러##\n",
    "\n",
    "def location_crawler():\n",
    "    \n",
    "    location_info_list=list()\n",
    "    \n",
    "    location_name_list = location_name_list_crawler()\n",
    "    # location_name_list =[ [지역1 ,지역1고유번호 , [ (고유번호,세부지역),(고유번호2,세부지역2)...] ],\n",
    "    #                       [지역2 ,지역2고유번호 , [ (고유번호,세부지역),(고유번호2,세부지역2)...] ],..\n",
    "    \n",
    "    for location in location_name_list:\n",
    "        location_name = location[0]\n",
    "        location_no = location[1]\n",
    "        sub_location_list = location[2]\n",
    "        for sub_location in sub_location_list:\n",
    "            print(sub_location)\n",
    "            sub_location_no = sub_location[0]\n",
    "            sub_location_name = sub_location[1]\n",
    "            sub_locations_info_list = pension_crawler(location_no, sub_location_no)\n",
    "            \n",
    "            #기존에 pension모델 1차적으로 이름,가격,이미지로  만들던것에\n",
    "            #location, sub_location 속성 추가해서 이곳에서 만들면 될듯하다.\n",
    "            # 일단은 리스트 형태로 정보 6개 묶어서 저장해보겠음.\n",
    "            #[[location,sub_location,title,pricem,img_file,pldx]....팬션 999까지 한 리스트에\n",
    "            \n",
    "            for i in range(len(sub_locations_info_list[0])):\n",
    "                location_info_list.append([location_name,\n",
    "                                           sub_location_name,\n",
    "                                           sub_locations_info_list[0][i],# title\n",
    "                                           sub_locations_info_list[1][i],# price\n",
    "                                           sub_locations_info_list[2][i],# img_file\n",
    "                                           sub_locations_info_list[3][i],# pldx\n",
    "                                           sub_locations_info_list[4][i]\n",
    "                                          ])\n",
    "    \n",
    "    return location_info_list\n",
    "    \n",
    "#########################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  해커톤때 모델저장하는것 크롤러에서 다해버렸는데 추후에 여기 참고하면 좋을듯.\n",
    "\n",
    "\n",
    "#  params = {\n",
    "#         'ypIdx': ypIdx\n",
    "#     }\n",
    "\n",
    "#     url = \"http://www.yapen.co.kr/details?\" + parse.urlencode(params)\n",
    "\n",
    "#     request = requests.get(url)\n",
    "#     response = request.text\n",
    "#     soup = BeautifulSoup(response, 'lxml')\n",
    "\n",
    "#     name_root = soup.select('div.wrap_1000')\n",
    "#     name = name_root[0].select('h3')[0].get_text()\n",
    "\n",
    "#     table = soup.select('table.pensionTbl')\n",
    "#     trs = table[0].select('tr')\n",
    "#     tds = trs[0].select('td')\n",
    "#     address = tds[0].get_text()\n",
    "\n",
    "#     tds2 = trs[1].select('td')\n",
    "#     check_in_out = tds2[0].select('span')\n",
    "#     check_in = check_in_out[0].get_text()\n",
    "#     check_out = check_in_out[1].get_text()\n",
    "\n",
    "#     td3 = trs[3].select('td')\n",
    "#     number_tags = td3[0].select('span')\n",
    "#     room = number_tags[0].get_text()\n",
    "\n",
    "#     td4 = trs[4].select('td')\n",
    "#     infos = td4[0].select('p')\n",
    "#     info_result = ''\n",
    "#     for info in infos:\n",
    "#         info_result = info_result + '\\n' + info.get_text() + '\\n'\n",
    "\n",
    "#     td5 = trs[5].select('td')\n",
    "#     lis = td5[0].select('li')\n",
    "#     theme_result = ''\n",
    "#     for li in lis:\n",
    "#         theme_result = theme_result + '\\n' + li.get_text() + '\\n'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "#@@@@@@@@@@@@@@@@@@@@@실행창 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    "# province_name_list = province_name_list_crawler()\n",
    "# print(province_name_list)\n",
    "\n",
    "# test = location_name_list_crawler()\n",
    "# print(test)\n",
    "\n",
    "# test = pension_crawler('015','1.015001')\n",
    "# print(test)\n",
    "\n",
    "\n",
    "# location_info_list = location_crawler()\n",
    "# print(location_info_list)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  province_dict  뽑는 과정인데 list로 바꾼후 여기다 일단 저장해 놓음. 나중에 필요하면 가져다 써라.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "# province_dict 뽑는 과정 \n",
    "\n",
    "def province_dict_crawler():\n",
    "    request = requests.get(\"http://www.yapen.co.kr\")\n",
    "    response = request.text\n",
    "    soup = BeautifulSoup(response, 'lxml')\n",
    "\n",
    "    left_menu=soup.select('div.locLayer')\n",
    "    # 풀빌라, MD추천 제외 14지역중 7지역 만남김.\n",
    "    selected_left_menu= left_menu[2:9]\n",
    "    # print(selected_left_menu)\n",
    "\n",
    "    # 여기에 dict 형태로 지역/(세부지역,고유번호) 넣고싶다. \n",
    "    # location_dict = {'지역':[(고유번호,세부지역),(고유번호2,세부지역2),(고유번호3,세부지역3)]}\n",
    "    location_dict=dict()\n",
    "\n",
    "    for location in selected_left_menu:\n",
    "        # 지역 이름 먼저 뽑음 \n",
    "        province_title = location.select('div.titleStyle')[0].get_text(strip=True)\n",
    "    #     print(province_title)\n",
    "\n",
    "        li=location.select('li')\n",
    "\n",
    "        #for문 돌면서 (고유번호 세부지역) 리스트에 담은뒤 province_dict에 넣겠다.\n",
    "        location_detail_list=[] \n",
    "        for location_detail in li:\n",
    "            onclick_value = location_detail['onclick']  #regionMove('1.003021','금산/논산');  \n",
    "            location_detail_tuple = tuple(re.findall(\"'(.+)'\",onclick_value))\n",
    "\n",
    "             # (고유번호 세부지역) 리스트에 담음\n",
    "            location_detail_list.append(location_detail_tuple)\n",
    "\n",
    "        # location_dict에 지역과 함께 (고유번호 세부지역) 리스트에 넣음.\n",
    "        location_dict[province_title]= location_detail_list\n",
    "        \n",
    "    return location_dict\n",
    "    \n",
    "    \n",
    "# province_dict 뽑는 과정 끝\n",
    "######################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
